# Health check manifests for DePIN AI Compute cluster
# These resources provide comprehensive cluster health monitoring

---
# Namespace for health monitoring
apiVersion: v1
kind: Namespace
metadata:
  name: cluster-health
  labels:
    name: cluster-health
    purpose: health-monitoring

---
# ServiceAccount for health checks
apiVersion: v1
kind: ServiceAccount
metadata:
  name: health-checker
  namespace: cluster-health

---
# ClusterRole for health checker
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: health-checker
rules:
- apiGroups: [""]
  resources: ["nodes", "pods", "services", "endpoints", "namespaces"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "daemonsets", "statefulsets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["nodes", "pods"]
  verbs: ["get", "list"]

---
# ClusterRoleBinding for health checker
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: health-checker
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: health-checker
subjects:
- kind: ServiceAccount
  name: health-checker
  namespace: cluster-health

---
# ConfigMap with health check scripts
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-check-scripts
  namespace: cluster-health
data:
  cluster-health.sh: |
    #!/bin/bash
    # Comprehensive cluster health check script
    
    set -euo pipefail
    
    GREEN='\033[0;32m'
    RED='\033[0;31m'
    YELLOW='\033[1;33m'
    NC='\033[0m'
    
    FAILED_CHECKS=0
    TOTAL_CHECKS=0
    
    log_info() {
        echo -e "${GREEN}[✓]${NC} $1"
    }
    
    log_warn() {
        echo -e "${YELLOW}[!]${NC} $1"
    }
    
    log_error() {
        echo -e "${RED}[✗]${NC} $1"
        ((FAILED_CHECKS++))
    }
    
    check_nodes() {
        echo "=== Node Health Check ==="
        ((TOTAL_CHECKS++))
        
        local ready_nodes=$(kubectl get nodes --no-headers | grep -c Ready || true)
        local not_ready_nodes=$(kubectl get nodes --no-headers | grep -c NotReady || true)
        local total_nodes=$(kubectl get nodes --no-headers | wc -l)
        
        if [[ $not_ready_nodes -eq 0 ]]; then
            log_info "All $ready_nodes nodes are Ready"
        else
            log_error "$not_ready_nodes out of $total_nodes nodes are NotReady"
            kubectl get nodes --no-headers | grep NotReady || true
        fi
        
        # Check node resource usage
        kubectl top nodes --no-headers 2>/dev/null || log_warn "Metrics server not available for node resource usage"
    }
    
    check_system_pods() {
        echo -e "\n=== System Pods Health Check ==="
        ((TOTAL_CHECKS++))
        
        local failed_pods=$(kubectl get pods -n kube-system --no-headers | grep -v Running | grep -v Completed | wc -l || true)
        
        if [[ $failed_pods -eq 0 ]]; then
            log_info "All system pods are running"
        else
            log_error "$failed_pods system pods are not running"
            kubectl get pods -n kube-system --no-headers | grep -v Running | grep -v Completed || true
        fi
        
        # Check critical system components
        local critical_components=("kube-apiserver" "etcd" "kube-controller-manager" "kube-scheduler" "coredns" "kube-proxy")
        for component in "${critical_components[@]}"; do
            if kubectl get pods -n kube-system | grep -q "$component"; then
                local running=$(kubectl get pods -n kube-system | grep "$component" | grep -c Running || true)
                if [[ $running -gt 0 ]]; then
                    log_info "$component: $running pod(s) running"
                else
                    log_error "$component: No running pods found"
                fi
            else
                log_warn "$component: No pods found (might be static pod)"
            fi
        done
    }
    
    check_cni() {
        echo -e "\n=== CNI Health Check ==="
        ((TOTAL_CHECKS++))
        
        # Check for CNI pods
        if kubectl get pods -A | grep -E "(calico|cilium|flannel|weave)" >/dev/null 2>&1; then
            local cni_pods_running=$(kubectl get pods -A | grep -E "(calico|cilium|flannel|weave)" | grep -c Running || true)
            local cni_pods_total=$(kubectl get pods -A | grep -E "(calico|cilium|flannel|weave)" | wc -l || true)
            
            if [[ $cni_pods_running -eq $cni_pods_total ]] && [[ $cni_pods_running -gt 0 ]]; then
                log_info "CNI: $cni_pods_running/$cni_pods_total pods running"
            else
                log_error "CNI: Only $cni_pods_running/$cni_pods_total pods running"
            fi
        else
            log_error "No CNI pods found"
        fi
    }
    
    check_dns() {
        echo -e "\n=== DNS Health Check ==="
        ((TOTAL_CHECKS++))
        
        # Check CoreDNS pods
        local coredns_running=$(kubectl get pods -n kube-system | grep coredns | grep -c Running || true)
        local coredns_total=$(kubectl get pods -n kube-system | grep coredns | wc -l || true)
        
        if [[ $coredns_running -eq $coredns_total ]] && [[ $coredns_running -gt 0 ]]; then
            log_info "CoreDNS: $coredns_running/$coredns_total pods running"
        else
            log_error "CoreDNS: Only $coredns_running/$coredns_total pods running"
        fi
        
        # Test DNS resolution
        if kubectl run dns-test --image=busybox --rm --restart=Never --timeout=30s -- nslookup kubernetes.default >/dev/null 2>&1; then
            log_info "DNS resolution test passed"
        else
            log_error "DNS resolution test failed"
        fi
    }
    
    check_api_server() {
        echo -e "\n=== API Server Health Check ==="
        ((TOTAL_CHECKS++))
        
        # Check API server responsiveness
        if kubectl get --raw="/healthz" >/dev/null 2>&1; then
            log_info "API server /healthz endpoint responding"
        else
            log_error "API server /healthz endpoint not responding"
        fi
        
        if kubectl get --raw="/readyz" >/dev/null 2>&1; then
            log_info "API server /readyz endpoint responding"
        else
            log_error "API server /readyz endpoint not responding"
        fi
        
        # Check API server components
        local api_health=$(kubectl get --raw="/healthz?verbose" 2>/dev/null | grep -c "ok" || true)
        if [[ $api_health -gt 0 ]]; then
            log_info "API server components health: $api_health checks passed"
        else
            log_warn "Unable to get detailed API server health"
        fi
    }
    
    check_etcd() {
        echo -e "\n=== etcd Health Check ==="
        ((TOTAL_CHECKS++))
        
        # Check etcd pods if running as pods (not static pods)
        local etcd_pods=$(kubectl get pods -n kube-system | grep etcd | wc -l || true)
        if [[ $etcd_pods -gt 0 ]]; then
            local etcd_running=$(kubectl get pods -n kube-system | grep etcd | grep -c Running || true)
            if [[ $etcd_running -eq $etcd_pods ]]; then
                log_info "etcd: $etcd_running/$etcd_pods pods running"
            else
                log_error "etcd: Only $etcd_running/$etcd_pods pods running"
            fi
        else
            log_info "etcd: Running as static pods (normal for kubeadm clusters)"
        fi
        
        # Test etcd connectivity through API server
        if kubectl get componentstatuses 2>/dev/null | grep -q "etcd.*Healthy"; then
            log_info "etcd connectivity test passed"
        else
            log_warn "etcd health check via componentstatuses failed (may be deprecated)"
        fi
    }
    
    check_storage() {
        echo -e "\n=== Storage Health Check ==="
        ((TOTAL_CHECKS++))
        
        # Check for storage classes
        local storage_classes=$(kubectl get storageclass --no-headers | wc -l || true)
        if [[ $storage_classes -gt 0 ]]; then
            log_info "Storage classes: $storage_classes available"
            kubectl get storageclass --no-headers | while read -r sc _; do
                echo "  - $sc"
            done
        else
            log_warn "No storage classes found"
        fi
        
        # Check for CSI drivers
        if kubectl get csidriver >/dev/null 2>&1; then
            local csi_drivers=$(kubectl get csidriver --no-headers | wc -l || true)
            log_info "CSI drivers: $csi_drivers available"
        else
            log_warn "No CSI drivers found or CSI not supported"
        fi
    }
    
    check_resource_usage() {
        echo -e "\n=== Resource Usage Check ==="
        ((TOTAL_CHECKS++))
        
        if command -v kubectl top >/dev/null && kubectl top nodes >/dev/null 2>&1; then
            echo "Node resource usage:"
            kubectl top nodes --no-headers | while read -r node cpu memory _; do
                cpu_percent=$(echo "$cpu" | sed 's/m//' | awk '{print $1/10}')
                memory_percent=$(echo "$memory" | sed 's/%//')
                
                if (( $(echo "$cpu_percent > 80" | bc -l) )); then
                    echo -e "  ${RED}[!]${NC} $node: CPU $cpu (high)"
                elif (( $(echo "$cpu_percent > 60" | bc -l) )); then
                    echo -e "  ${YELLOW}[!]${NC} $node: CPU $cpu (moderate)"
                else
                    echo -e "  ${GREEN}[✓]${NC} $node: CPU $cpu"
                fi
                
                if (( memory_percent > 80 )); then
                    echo -e "  ${RED}[!]${NC} $node: Memory $memory (high)"
                elif (( memory_percent > 60 )); then
                    echo -e "  ${YELLOW}[!]${NC} $node: Memory $memory (moderate)"
                else
                    echo -e "  ${GREEN}[✓]${NC} $node: Memory $memory"
                fi
            done
        else
            log_warn "Metrics server not available for resource usage check"
        fi
    }
    
    # Main execution
    echo "Starting DePIN AI Compute Cluster Health Check..."
    echo "Timestamp: $(date)"
    echo
    
    check_nodes
    check_system_pods
    check_cni
    check_dns
    check_api_server
    check_etcd
    check_storage
    check_resource_usage
    
    echo -e "\n=== Health Check Summary ==="
    echo "Total checks performed: $TOTAL_CHECKS"
    echo "Failed checks: $FAILED_CHECKS"
    
    if [[ $FAILED_CHECKS -eq 0 ]]; then
        log_info "All health checks passed!"
        exit 0
    else
        log_error "$FAILED_CHECKS health checks failed"
        exit 1
    fi

---
# CronJob to run periodic health checks
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cluster-health-check
  namespace: cluster-health
spec:
  schedule: "*/15 * * * *"  # Run every 15 minutes
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: cluster-health-check
        spec:
          serviceAccountName: health-checker
          restartPolicy: OnFailure
          containers:
          - name: health-checker
            image: bitnami/kubectl:latest
            command:
            - /bin/bash
            - /scripts/cluster-health.sh
            volumeMounts:
            - name: health-scripts
              mountPath: /scripts
            resources:
              limits:
                memory: 256Mi
                cpu: 100m
              requests:
                memory: 128Mi
                cpu: 50m
          volumes:
          - name: health-scripts
            configMap:
              name: health-check-scripts
              defaultMode: 0755

---
# Service for health check metrics exposure
apiVersion: v1
kind: Service
metadata:
  name: cluster-health-metrics
  namespace: cluster-health
  labels:
    app: cluster-health-check
spec:
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  selector:
    app: cluster-health-check

---
# Deployment for health check web UI (optional)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: health-dashboard
  namespace: cluster-health
spec:
  replicas: 1
  selector:
    matchLabels:
      app: health-dashboard
  template:
    metadata:
      labels:
        app: health-dashboard
    spec:
      serviceAccountName: health-checker
      containers:
      - name: dashboard
        image: nginx:alpine
        ports:
        - containerPort: 80
        resources:
          limits:
            memory: 128Mi
            cpu: 100m
          requests:
            memory: 64Mi
            cpu: 50m
        volumeMounts:
        - name: dashboard-config
          mountPath: /etc/nginx/conf.d
        - name: dashboard-html
          mountPath: /usr/share/nginx/html
      volumes:
      - name: dashboard-config
        configMap:
          name: health-dashboard-config
      - name: dashboard-html
        configMap:
          name: health-dashboard-html

---
# ConfigMap for dashboard HTML
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-dashboard-html
  namespace: cluster-health
data:
  index.html: |
    <!DOCTYPE html>
    <html>
    <head>
        <title>DePIN AI Compute - Cluster Health</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            .header { background: #2196F3; color: white; padding: 20px; }
            .status { margin: 20px 0; }
            .healthy { color: green; }
            .warning { color: orange; }
            .error { color: red; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>DePIN AI Compute Cluster Health Dashboard</h1>
        </div>
        <div class="status">
            <p>This is a basic health dashboard. For detailed health information, run:</p>
            <code>kubectl logs -n cluster-health job/cluster-health-check-&lt;timestamp&gt;</code>
        </div>
    </body>
    </html>

---
# ConfigMap for nginx configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-dashboard-config
  namespace: cluster-health
data:
  default.conf: |
    server {
        listen 80;
        server_name localhost;
        location / {
            root /usr/share/nginx/html;
            index index.html;
        }
    }